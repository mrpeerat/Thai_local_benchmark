{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_from_disk\n",
    "from datasets import DatasetDict\n",
    "import datasets\n",
    "\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/aisg/peerat/Thai_local_benchmark/datasets/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_name = {\n",
    "    \"esan\": [\"XX\",\"XX\"],\n",
    "    \"north\": [\"XX\",\"XX\"],\n",
    "    \"south\": [\"XX\",\"XX\"],\n",
    "    \"center\": [\"XX\",\"XX\"],\n",
    "}\n",
    "\n",
    "conversation_list = [\n",
    "    \"XXXX\", \n",
    "    \"XXXX\",\n",
    "    \"XXXX\",\n",
    "    \"XXXX\",\n",
    "    \"XXXX\",\n",
    "    \"XXXX\",\n",
    "    \"XXXX\",\n",
    "    \"XXXX\",\n",
    "    \"XXXX\",\n",
    "    \"XXXX\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data = DatasetDict()\n",
    "task_data = DatasetDict()\n",
    "local_data = DatasetDict()\n",
    "\n",
    "for local in [\"esan\",\"north\",\"south\",\"center\"]:\n",
    "    for file in glob(path):\n",
    "        task_name = file.split(\"/\")[-1].split(\".\")[0]\n",
    "        df = pd.read_csv(file)\n",
    "        if task_name == 'qa':\n",
    "            new_df = df[[\"id\",\"question\",'text_2','context']]\n",
    "            dataset = Dataset.from_pandas(new_df)\n",
    "        elif task_name == 'summarization':\n",
    "            new_df = df[[\"id\",\"text_2\",\"text_1\",\"title\",\"url\"]]\n",
    "            dataset = Dataset.from_pandas(new_df)\n",
    "        elif 'translation' in task_name:\n",
    "            new_df = df[[\"id\",\"text_2\",\"text_1\"]]\n",
    "            dataset = Dataset.from_pandas(new_df)\n",
    "        task_data[task_name] = dataset\n",
    "\n",
    "    # conversation\n",
    "    task_name='conversation'\n",
    "    new_df = pd.DataFrame(list(zip(list(range(len(conversation_list))), conversation_list)),columns =['id', 'text1'])\n",
    "    dataset = Dataset.from_pandas(new_df)\n",
    "    task_data[task_name] = dataset\n",
    "\n",
    "    task_name='food'\n",
    "    food_list = food_name[local]\n",
    "    new_df = pd.DataFrame(list(zip(list(range(len(food_list))), food_list)),columns =['id', 'text1'])\n",
    "    dataset = Dataset.from_pandas(new_df)\n",
    "    task_data[task_name] = dataset\n",
    "\n",
    "    local_data[local] = task_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/dataset.pkl', 'wb') as file:  \n",
    "    # A new file will be created \n",
    "    pickle.dump(local_data, file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    esan: DatasetDict({\n",
      "        qa: Dataset({\n",
      "            features: ['id', 'question', 'text_2', 'context'],\n",
      "            num_rows: 20\n",
      "        })\n",
      "        summarization: Dataset({\n",
      "            features: ['id', 'text_2', 'text_1', 'title', 'url'],\n",
      "            num_rows: 20\n",
      "        })\n",
      "        translation_eng_esan: Dataset({\n",
      "            features: ['id', 'text_2', 'text_1'],\n",
      "            num_rows: 20\n",
      "        })\n",
      "        translation_esan_eng: Dataset({\n",
      "            features: ['id', 'text_2', 'text_1'],\n",
      "            num_rows: 20\n",
      "        })\n",
      "        conversation: Dataset({\n",
      "            features: ['id', 'text1'],\n",
      "            num_rows: 10\n",
      "        })\n",
      "        food: Dataset({\n",
      "            features: ['id', 'text1'],\n",
      "            num_rows: 2\n",
      "        })\n",
      "    })\n",
      "    north: DatasetDict({\n",
      "        qa: Dataset({\n",
      "            features: ['id', 'question', 'text_2', 'context'],\n",
      "            num_rows: 20\n",
      "        })\n",
      "        summarization: Dataset({\n",
      "            features: ['id', 'text_2', 'text_1', 'title', 'url'],\n",
      "            num_rows: 20\n",
      "        })\n",
      "        translation_eng_esan: Dataset({\n",
      "            features: ['id', 'text_2', 'text_1'],\n",
      "            num_rows: 20\n",
      "        })\n",
      "        translation_esan_eng: Dataset({\n",
      "            features: ['id', 'text_2', 'text_1'],\n",
      "            num_rows: 20\n",
      "        })\n",
      "        conversation: Dataset({\n",
      "            features: ['id', 'text1'],\n",
      "            num_rows: 10\n",
      "        })\n",
      "        food: Dataset({\n",
      "            features: ['id', 'text1'],\n",
      "            num_rows: 2\n",
      "        })\n",
      "    })\n",
      "    south: DatasetDict({\n",
      "        qa: Dataset({\n",
      "            features: ['id', 'question', 'text_2', 'context'],\n",
      "            num_rows: 20\n",
      "        })\n",
      "        summarization: Dataset({\n",
      "            features: ['id', 'text_2', 'text_1', 'title', 'url'],\n",
      "            num_rows: 20\n",
      "        })\n",
      "        translation_eng_esan: Dataset({\n",
      "            features: ['id', 'text_2', 'text_1'],\n",
      "            num_rows: 20\n",
      "        })\n",
      "        translation_esan_eng: Dataset({\n",
      "            features: ['id', 'text_2', 'text_1'],\n",
      "            num_rows: 20\n",
      "        })\n",
      "        conversation: Dataset({\n",
      "            features: ['id', 'text1'],\n",
      "            num_rows: 10\n",
      "        })\n",
      "        food: Dataset({\n",
      "            features: ['id', 'text1'],\n",
      "            num_rows: 2\n",
      "        })\n",
      "    })\n",
      "    center: DatasetDict({\n",
      "        qa: Dataset({\n",
      "            features: ['id', 'question', 'text_2', 'context'],\n",
      "            num_rows: 20\n",
      "        })\n",
      "        summarization: Dataset({\n",
      "            features: ['id', 'text_2', 'text_1', 'title', 'url'],\n",
      "            num_rows: 20\n",
      "        })\n",
      "        translation_eng_esan: Dataset({\n",
      "            features: ['id', 'text_2', 'text_1'],\n",
      "            num_rows: 20\n",
      "        })\n",
      "        translation_esan_eng: Dataset({\n",
      "            features: ['id', 'text_2', 'text_1'],\n",
      "            num_rows: 20\n",
      "        })\n",
      "        conversation: Dataset({\n",
      "            features: ['id', 'text1'],\n",
      "            num_rows: 10\n",
      "        })\n",
      "        food: Dataset({\n",
      "            features: ['id', 'text1'],\n",
      "            num_rows: 2\n",
      "        })\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "with open('datasets/dataset.pkl', 'rb') as file: \n",
    "      \n",
    "    # Call load method to deserialze \n",
    "    loaded_file = pickle.load(file) \n",
    "  \n",
    "    print(loaded_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peerat_local",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
